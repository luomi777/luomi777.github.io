<!doctype html>
<html>
  <head>
    <title>Romy Mi Luo's Homepage</title>

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <meta name="description" content="Mi Luo is at UT Austin.">
    <meta name="keywords" content="Mi Luo, romy luo, luomi">

    <meta property="og:type" content="website">
    <meta property="og:title" content="Mi Luo's Homepage">
    <meta property="og:site_name" content="Mi Luo's Homepage">
    <meta property="og:description" content="Mi Luo is at UT Austin.">
    <meta property="og:locale" content="default">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Mi Luo's Homepage">
    <meta name="twitter:description" content="Mi Luo is at UT Austin.">


    <link rel="icon" type="image/png" sizes="32x32" href="favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="favicon/favicon-16x16.png">

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.1/css/all.css" integrity="sha384-gfdkjb5BdAXd+lj+gudLWI+BXq4IuLW5IT+brZEZsLFm++aCMlF1V92rMkPaX4PP" crossorigin="anonymous">
    <link rel="stylesheet" href="stylesheets/academicons/css/academicons.min.css"/>


  </head>
  <body>
    <div class="wrapper">
      <header>
        <center>
        <a href="#" class="image avatar"><img src="images/avatar.jpg" alt="" onContextMenu="return false" /></a>
        <h1><strong>(Romy) Mi LUO</strong><br></h1>
            University of Texas at Austin<br>
            romyluo7 (at) gmail.com<br>
            [<a href="https://scholar.google.com/citations?user=eL-xIlAAAAAJ&hl=en&oi=ao" target="_blank" rel="noopener">Google Scholar</a>]
            <br>
        </center>
      </header>
      <section>
          <h1 id="biography"><a href="#biography" class="headerlink" title="biography"></a>About Me</h1>

              <p>I am a third-year PhD student at UT Austin, advised by <a href="https://www.cs.utexas.edu/users/grauman/" target="_blank" rel="noopener">Prof. Kristen Grauman</a> and <a href="https://people.eecs.berkeley.edu/~alexdimakis/index.html" target="_blank" rel="noopener">Prof. Alex Dimakis</a>. My research lies in <strong>Machine Learning</strong> and <strong>Computer Vision</strong>, specifically in the following topics:</p>
              <ul>
              <li> Video Generation
              <li> Multimodal LLMs for Video Understanding
              <li> 3D/4D Modeling from Videos
              </ul>
          <h1 id="publications"><a href="#publications" class="headerlink" title="publications"></a>Publications & Manuscripts</h1>
          <ul>
              <li><papertitle>Viewpoint Rosetta Stone: Unlocking Unpaired Ego-Exo Videos for View-invariant Representation Learning</papertitle>
                <br>
                <strong>Mi Luo</strong>, Zihui Xue, Alex Dimakis, Kristen Grauman
                <br>
                In IEEE/CVF Conference on Computer Vision and Pattern Recognition, <strong>CVPR 2025</strong>. (<strong>Oral</strong>)
                <br>
                [<a href="images/vrs_cvpr.pdf" target="_blank" rel="noopener">PDF</a>][<a href="https://vision.cs.utexas.edu/projects/ViewpointRosetta/" target="_blank" rel="noopener">Project</a>]          

                </ul>
        
              <ul>
              <li><papertitle>HOI-Swap: Swapping Objects in Videos with Hand-Object Interaction Awareness</papertitle>
                <br>
                Zihui Xue, <strong>Mi Luo</strong>, Changan Chen, Kristen Grauman
                <br>
                In Advances in Neural Information Processing Systems, <strong>NeurIPS 2024</strong>. 
                <br>
                [<a href="https://arxiv.org/abs/2406.07754" target="_blank" rel="noopener">PDF</a>][<a href="https://vision.cs.utexas.edu/projects/HOI-Swap/" target="_blank" rel="noopener">Project</a>]
                </ul>

              <ul>
              <li><papertitle>4DIFF: 3D-Aware Diffusion Model for Third-to-First Viewpoint Translation</papertitle>
                <br>
                Feng Cheng*, <strong>Mi Luo*</strong>, Huiyu Wang, Alex Dimakis, Lorenzo Torresani, Gedas Bertasius, Kristen Grauman
                <br>
                In 2024 European Conference on Computer Vision, <strong>ECCV 2024</strong>.
                <br>
                [<a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03536.pdf" target="_blank" rel="noopener">PDF</a>][<a href="https://klauscc.github.io/4diff" target="_blank" rel="noopener">Project</a>]
                </ul>
        
              <ul>
              <li><papertitle>Put Myself in Your Shoes: Lifting the Egocentric Perspective from Exocentric Videos</papertitle>
                <br>
                <strong>Mi Luo</strong>, Zihui Xue, Alex Dimakis, Kristen Grauman
                <br>
                In 2024 European Conference on Computer Vision, <strong>ECCV 2024</strong>.
                <br>
                [<a href="https://arxiv.org/pdf/2403.06351.pdf" target="_blank" rel="noopener">PDF</a>][<a href="https://arxiv.org/pdf/2403.06351.pdf" target="_blank" rel="noopener">Project</a>]
                </ul>

        
              <ul>
              <li><papertitle>Ego-Exo4D: Understanding Skilled Human Activity from First-and Third-Person Perspectives</papertitle>
                <br>
                Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, …, <strong>Mi Luo</strong>, …, Pablo Arbelaez, Gedas Bertasius, David Crandall, Dima Damen, Jakob Engel, Giovanni Maria Farinella, Antonino Furnari, Bernard Ghanem, Judy Hoffman, C. V. Jawahar, Richard Newcombe, Hyun Soo Park, James M. Rehg, Yoichi Sato, Manolis Savva, Jianbo Shi, Mike Zheng Shou, Michael Wray
                <br>
                In IEEE/CVF Conference on Computer Vision and Pattern Recognition, <strong>CVPR 2024</strong>. (<strong>Oral</strong>)
                <br>
                [<a href="https://arxiv.org/pdf/2311.18259.pdf" target="_blank" rel="noopener">PDF</a>][<a href="https://ego-exo4d-data.org/" target="_blank" rel="noopener">Project</a>][<a href="https://ai.meta.com/blog/ego-exo4d-video-learning-perception/" target="_blank" rel="noopener">Overview</a>][<a href="https://www.youtube.com/watch?v=GdooXEBAnI8" target="_blank" rel="noopener">Video</a>]
                </ul>
        
              <ul>
              <li><papertitle>MetaFormer Baselines for Vision</papertitle>
                <br>
                Weihao Yu, Chenyang Si, Pan Zhou, <strong>Mi Luo</strong>, Yichen Zhou, Jiashi Feng, Shuicheng Yan, Xinchao Wang
                <br>
                IEEE Transactions on Pattern Analysis and Machine Intelligence, <strong>T-PAMI 2023</strong>.
                <br>
                [<a href="https://arxiv.org/pdf/2210.13452.pdf" target="_blank" rel="noopener">PDF</a>][<a href="https://github.com/sail-sg/metaformer" target="_blank" rel="noopener">Project</a>]
                </ul>
        
              <ul>
              <li><papertitle>MetaFormer is Actually What You Need for Vision</papertitle>
                <br>
                Weihao Yu, <strong>Mi Luo</strong>, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, Shuicheng Yan
                <br>
                In IEEE/CVF Conference on Computer Vision and Pattern Recognition, <strong>CVPR 2022</strong>. (<strong>Oral</strong>)
                <br>
                [<a href="https://arxiv.org/pdf/2111.11418.pdf" target="_blank" rel="noopener">PDF</a>][<a href="https://github.com/sail-sg/poolformer" target="_blank" rel="noopener">Project</a>]
                <br>
                </ul>
        
              <ul>
              <li><papertitle>Architecture Personalization in Resource-constrained Federated Learning</papertitle>
                <br>
                <strong>Mi Luo</strong>, Fei Chen, Zhenguo Li, Jiashi Feng
                <br>
                In <strong><a href="https://neurips2021workshopfl.github.io/NFFL-2021/index.html" target="_blank" rel="noopener">NFFL Workshop</a></strong>, <strong>NeurIPS 2021</strong>. (<strong>Selected as outstanding paper, acceptance rate: 9%</strong>)
                <br>
                [<a href="images/Workshop-version_Camera ready.pdf" target="_blank" rel="noopener">PDF</a>]                
                <br>
              </ul>
        
        
              <ul>
              <li><papertitle>No Fear of Heterogeneity: Classifier Calibration for Federated Learning with Non-IID Data</papertitle>
                <br>
                <strong>Mi Luo</strong>, Fei Chen, Dapeng Hu, Yifan Zhang, Jian Liang, Jiashi Feng
                <br>
                In Advances in Neural Information Processing Systems, <strong>NeurIPS 2021</strong>.	
                <br>
                [<a href="https://arxiv.org/abs/2106.05001" target="_blank" rel="noopener">PDF</a>]
                <br>
              </ul>
        
              <ul>
              <li><papertitle>MetaSelector: Meta-Learning for Recommendation with User-Level Adaptive Model Selection</papertitle>
                <br>
                <strong>Mi Luo</strong>, Fei Chen, Pengxiang Cheng, Zhenhua Dong, Xiuqiang He, Jiashi Feng, Zhenguo Li
                <br>
                In Proceedings of The Web Conference, <strong>WWW 2020</strong>.
                <br>
                [<a href="https://dl.acm.org/doi/fullHtml/10.1145/3366423.3379999" target="_blank" rel="noopener">PDF</a>]
              </li>
              </ul>
        

        <h1 id="research"><a href="#research" class="headerlink" title="research"></a>Research Experiences</h1>
             <ul>
              <li><p style="float:right">Menlo Park, May 2024 - Dec 2024</p> <strong>FAIR, Meta AI</strong>
                <br>
                Research Intern, Topics: data curation and video encoder design for multimodal LLMs</a>
              </ul>
        
              <ul>
              <li><p style="float:right">Singapore, Sep 2021 - Apr 2022</p> <strong>SEA AI Lab (SAIL)</strong>
                <br>
                Research Intern, Topic: vision transformer architecture</a>
              </ul>

              <ul>
              <li><p style="float:right">Singapore, Aug 2020 - Aug 2021</p> <strong>National University of Singapore </strong>
                <br>
                Research Assistant, Topics: federated learning, machine learning with non-IID data</a>
              </ul>
        

        <h1 id="service"><a href="#service" class="headerlink" title="service"></a>Professional Service</h1>
              <ul>
              <li><strong>Conference Reviewer</strong>: CVPR 2023 - 2024, ECCV 2024, ICCV 2023, ACCV 2024, NeurIPS 2023-2024, ICLR 2024-2025, ICML 2024-2025, AISTATS 2022-2025, AAAI 2025
                <br>
              <li><strong>Journal Reviewer</strong>: International Journal of Computer Vision (IJCV), IEEE Transactions on Knowledge and Data Engineering (TKDE), Transactions on Machine Learning Research (TMLR)
              <br>
              <li><strong>Teaching Assistant</strong>: EE2211 (Introduction to Machine Learning), CG3207 (Computer Architecture)
              </ul>
        
          <br>

      </section>

      <footer>
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=d3ebed&w=300&t=n&d=_RtXcKM0NfeqVtamIco4ZifjHtbkiQqlat-tseipi9k&co=ffffff&cmo=c1eaad&cmn=5caa5c&ct=808080'></script>  </body>
</html>
